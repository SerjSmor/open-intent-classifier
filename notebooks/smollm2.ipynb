{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a8142e2-4f4a-48c9-bd1a-82a596c08fbe",
   "metadata": {},
   "source": [
    "# 1.7B Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "299d5cd3-36e9-49f7-87e7-5802e1c6b75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# checkpoint = \"HuggingFaceTB/SmolLM2-360M\"\n",
    "# checkpoint = \"HuggingFaceTB/SmolLM2-360M-Instruct\"\n",
    "def get_model_tokenizer(checkpoint):\n",
    "    device = \"cuda\" # for GPU usage or \"cpu\" for CPU usage\n",
    "    tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "    # for multiple GPUs install accelerate and do `model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\")`\n",
    "    model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\n",
    "    return model, tokenizer\n",
    "\n",
    "big_model = \"HuggingFaceTB/SmolLM2-1.7B-Instruct\"\n",
    "small_model = \"HuggingFaceTB/SmolLM2-360M-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d8d0e81-7ea0-4629-8fa0-c49cbdae7a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = get_model_tokenizer(big_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b837c84f-4d1d-4988-bad6-83bc899e0fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "/home/serj/dev/open-intent-classifier/venv/lib/python3.10/site-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gravity is a fundamental force of nature that affects all objects with mass, from the smallest subatomic\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "inputs = tokenizer.encode(\"Gravity is\", return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(inputs)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ba388418-8b1a-48ef-9beb-18f6f83ac135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the chat template\n",
    "prompt_template = '''You are a customer service expert. Your goal is to predict what is the intent of the user from a predfined list.\n",
    "    Format, the list of intents is delimeted with %.\n",
    "    You must choose only one\n",
    "    Text: {text}\n",
    "    Intent List: {labels}.\n",
    "\n",
    "    Answer format: {class: }\n",
    "    Answer: \n",
    "    '''\n",
    "\n",
    "# def predict(text, labels, device=\"cuda\"):    \n",
    "#     labels_str = \"%\".join(labels)\n",
    "#     prompt = prompt_template.replace(\"{text}\", text).replace(\"{labels}\", labels_str)\n",
    "#     print(prompt)\n",
    "#     inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "#     outputs = model.generate(inputs, max_new_tokens=200)\n",
    "#     return tokenizer.decode(outputs[0])\n",
    "\n",
    "def predict_roles(text, labels, device=\"cuda\"):\n",
    "    \n",
    "    labels_str = \"%\".join(labels)\n",
    "\n",
    "    prompt_format = '''Format, the list of intents is delimeted with %.\n",
    "    You must choose only one\n",
    "    Text: {text}\n",
    "    Intent List: {labels}.\n",
    "    Answer format is json: {class: }\n",
    "    You must follow this format'''\n",
    "    prompt = prompt_format.replace(\"{text}\", text).replace(\"{labels}\", labels_str)\n",
    "    messages = [{\"role\": \"system\", \"content\": \"You are a customer service expert. Your goal is to predict what is the intent of the user from a predfined list\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}]\n",
    "    \n",
    "    input_text=tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "    inputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(inputs, max_new_tokens=50)\n",
    "    return tokenizer.decode(outputs[0])\n",
    "\n",
    "def predict_small_model(text, labels, device=\"cuda\"):\n",
    "    labels_str = \"%\".join(labels)\n",
    "\n",
    "    prompt_format = '''Format, the list of intents is delimeted with %.\n",
    "    Text: is the text of the customer.\n",
    "    Intent List: is the list of possible intents for the text. You need to choose one.\n",
    "    You must choose only one\n",
    "    --\n",
    "    Your answer must include only the name of the class, without any prefix, suffix or inbetween nicities.\n",
    "    You must follow this format.\n",
    "    -- \n",
    "    Here is an example:\n",
    "    Text: I have an issues with my iPhone\n",
    "    Intent List: technical issue % cancel subscription\n",
    "    Class: technical issue\n",
    "    --\n",
    "    Text: {text}\n",
    "    Intent List: {labels}.\n",
    "    Class: '''\n",
    "    prompt = prompt_format.replace(\"{text}\", text).replace(\"{labels}\", labels_str)\n",
    "    messages = [{\"role\": \"system\", \"content\": \"You are a customer service expert. Your goal is to predict what is the intent of the user from a predfined list\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}]\n",
    "    \n",
    "    input_text=tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "    inputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(inputs, max_new_tokens=100)\n",
    "    return tokenizer.decode(outputs[0])\n",
    "\n",
    "\n",
    "def predict_small_model_v2(text, labels, device=\"cuda\"):\n",
    "    labels_str = \"%\".join(labels)\n",
    "\n",
    "    # prompt_format = '''Format, the list of intents is delimeted with %.\n",
    "    # Text: is the text of the customer.\n",
    "    # Intent List: is the list of possible intents for the text. You need to choose one.\n",
    "    # You must choose only one\n",
    "    # --\n",
    "    # Your answer must include only the name of the class, without any prefix, suffix or inbetween nicities.\n",
    "    # You must follow this format.\n",
    "    # -- \n",
    "    # Here is an example:\n",
    "    # Text: I have an issues with my iPhone\n",
    "    # Intent List: technical issue % cancel subscription\n",
    "    # Class: technical issue\n",
    "    # --\n",
    "    # Text: {text}\n",
    "    # Intent List: {labels}.\n",
    "    # Class: '''\n",
    "    \n",
    "    \n",
    "    # prompt = prompt_format.replace(\"{text}\", text).replace(\"{labels}\", labels_str)\n",
    "    prompt = f\"Customer Text: {text}. Most matching intent from list: \"\n",
    "    messages = [{\"role\": \"system\", \"content\": f\"You are a customer service expert. Your goal is to predict what is the intent of the user from a predfined list: {labels_str}. Do not return any text other than the text of the most matching intent from the list.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}]\n",
    "    \n",
    "    input_text=tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "    inputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(inputs, max_new_tokens=100)\n",
    "    return tokenizer.decode(outputs[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78119cd1-de18-4273-bf2c-33912f36f918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a customer service expert. Your goal is to predict what is the intent of the user from a predfined list.\n",
      "    Format, the list of intents is delimeted with %.\n",
      "    You must choose only one\n",
      "    Text: Hey, I want to cancel subscription\n",
      "    Intent List: cancel subscription%refund requests%bug%issue%unknown.\n",
      "\n",
      "    Answer format: {class: }\n",
      "    Answer: \n",
      "    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'You are a customer service expert. Your goal is to predict what is the intent of the user from a predfined list.\\n    Format, the list of intents is delimeted with %.\\n    You must choose only one\\n    Text: Hey, I want to cancel subscription\\n    Intent List: cancel subscription%refund requests%bug%issue%unknown.\\n\\n    Answer format: {class: }\\n    Answer: \\n    1. cancel subscription\\n    2. refund requests\\n    3. bug\\n    4. issue\\n    5. unknown\\n\\nText: I want to cancel my subscription\\nIntent List: cancel subscription%refund requests%bug%issue%unknown.\\n\\n    Answer format: {class: }\\n    Answer: \\n    1. cancel subscription\\n    2. refund requests\\n    3. bug\\n    4. issue\\n    5. unknown\\n\\nText: I want to cancel my subscription\\nIntent List: cancel subscription%refund requests%bug%issue%unknown.\\n\\n    Answer format: {class: }\\n    Answer: \\n    1. cancel subscription\\n    2. refund requests\\n    3. bug\\n    4. issue\\n    5. unknown\\n\\nText: I want to cancel my subscription\\nIntent List: cancel subscription%refund requests%bug%issue%unknown.\\n\\n    Answer format: {class: }\\n    Answer: \\n    1. cancel subscription\\n    2. refund requests\\n    3. bug\\n    '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Hey, I want to cancel subscription\"\n",
    "labels = [\"cancel subscription\", \"refund requests\", \"bug\", \"issue\", \"unknown\"]\n",
    "predict(text, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d89e9619-0bbe-40a0-be94-1ac02f1358e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nYou are a customer service expert. Your goal is to predict what is the intent of the user from a predfined list<|im_end|>\\n<|im_start|>user\\nFormat, the list of intents is delimeted with %.\\n    You must choose only one\\n    Text: Hey, I want to cancel subscription\\n    Intent List: cancel subscription%refund requests%bug%issue%unknown.\\n    Answer format is json: {class: }\\n    You must follow this format<|im_end|>\\n<|im_start|>assistant\\n{\\n    \"class\": \"cancel_subscription\"\\n}<|im_end|>'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_roles(text, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e82ac4a8-a30d-4b86-b24a-f3eafc839b35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nYou are a customer service expert. Your goal is to predict what is the intent of the user from a predfined list<|im_end|>\\n<|im_start|>user\\nFormat, the list of intents is delimeted with %.\\n    You must choose only one\\n    Text: I hate this job and your service, I have to get my money back\\n    Intent List: cancel subscription%refund requests%bug%issue%unknown.\\n    Answer format is json: {class: }\\n    You must follow this format<|im_end|>\\n<|im_start|>assistant\\n{\\n    \"class\": \"refund_requests\"\\n}<|im_end|>'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_roles(\"I hate this job and your service, I have to get my money back\", labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c66a6e3-2d4d-43ae-90c6-a63badd8fa5d",
   "metadata": {},
   "source": [
    "# 360M Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6d24c4c-bd0b-4632-b116-cfd4b74eab21",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = get_model_tokenizer(small_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "08fbfb4c-c8ee-47a2-b3ec-df3a70fbcc25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nYou are a customer service expert. Your goal is to predict what is the intent of the user from a predfined list: cancel subscription%refund requests%bug%issue%unknown. Do not return any text other than the text of the most matching intent from the list.<|im_end|>\\n<|im_start|>user\\nCustomer Text: Hey, I want to cancel subscription. Most matching intent from list: <|im_end|>\\n<|im_start|>assistant\\nCancel subscription.<|im_end|>'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_small_model_v2(text, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "06a28a7a-51fa-4010-ba06-80d9177e4a5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|im_start|>system\\nYou are a customer service expert. Your goal is to predict what is the intent of the user from a predfined list: cancel subscription%refund requests%bug%issue%unknown. Do not return any text other than the text of the most matching intent from the list.<|im_end|>\\n<|im_start|>user\\nCustomer Text: I hate this job and your service, I have to get my money back. Most matching intent from list: <|im_end|>\\n<|im_start|>assistant\\nI apologize for the inconvenience. I'm sorry to hear that you're having issues with your service. I'm here to help. Can you please provide more details about the issue you're experiencing?<|im_end|>\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_small_model_v2(\"I hate this job and your service, I have to get my money back\", labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34634a1-ebef-464c-b9f1-1292e76f2a31",
   "metadata": {},
   "source": [
    "# 1.7B with DSPY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97c9d700-9ea1-4532-ad45-1474cf9f502d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "accelerate.big_modeling - WARNING - Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "import dspy\n",
    "from open_intent_classifier.model import Classification\n",
    "\n",
    "llm = dspy.HFModel(model=big_model)\n",
    "dspy.settings.configure(lm=llm)\n",
    "predict = dspy.Predict(Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e6ffc8c-754e-447b-b606-455425338c2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Please answer. what is gravity?']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"Please answer. what is gravity?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92626ce7-105c-4d50-b46f-972c8a7186ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/serj/dev/open-intent-classifier/venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    intent_class=\"Classify the customer message into one of the intent labels.\\nThe output should be only the predicted class as a single intent label.\\n\\n---\\n\\nFollow the following format.\\n\\nCustomer Message: Customer message during customer service interaction\\nIntent Labels: Labels that represent customer intent\\nIntent Class: a label best matching customer's intent\\n\\n---\\n\\nCustomer Message: I want to cancel my subscription\\nIntent Labels: Cancel subscription Refund request\\nIntent Class: Cancel subscription\\n\\nCustomer Message: I want to change my subscription plan\\nIntent Labels: Change subscription plan Refund request\\nIntent Class: Change subscription plan\\n\\nCustomer Message: I want to cancel my subscription and get a refund\\nIntent Labels: Cancel subscription Refund request\\nIntent Class: Cancel subscription\\n\\nCustomer Message: I want to change my subscription plan and get a refund\\nIntent Labels: Change subscription plan Refund request\\nIntent Class: Change subscription plan\\n\\nCustomer Message: I want to cancel my subscription and get a refund, but I want to keep my account\\nIntent Labels: Cancel subscription Refund request\\nIntent Class: Cancel subscription\"\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict = dspy.Predict(Classification)\n",
    "labels = ([\"Cancel subscription\", \"Refund request\"])\n",
    "labels = \" \".join([\"Cancel subscription\", \"Refund request\"])\n",
    "text = \"I want to cancel my subscription\"\n",
    "pred = predict(customer_message=text, intent_labels=labels)\n",
    "pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655a933f-dcb8-4d18-88b5-9dc8792fb2cd",
   "metadata": {},
   "source": [
    "# 360M with DSPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64956afb-876b-44cf-b298-579141b9a224",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    }
   ],
   "source": [
    "import dspy\n",
    "from open_intent_classifier.model import Classification\n",
    "\n",
    "llm = dspy.HFModel(model=small_model)\n",
    "dspy.settings.configure(lm=llm)\n",
    "predict = dspy.Predict(Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1518566f-78f3-4221-9499-c0700cbd70df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "root - WARNING - \t*** In DSPy 2.5, all LM clients except `dspy.LM` are deprecated, underperform, and are about to be deleted. ***\n",
      " \t\tYou are using the client HFModel, which will be removed in DSPy 2.6.\n",
      " \t\tChanging the client is straightforward and will let you use new features (Adapters) that improve the consistency of LM outputs, especially when using chat LMs. \n",
      "\n",
      " \t\tLearn more about the changes and how to migrate at\n",
      " \t\thttps://github.com/stanfordnlp/dspy/blob/main/examples/migration.ipynb\n",
      "/home/serj/dev/open-intent-classifier/venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    intent_class=\"Classify the customer message into one of the intent labels.\\nThe output should be only the predicted class as a single intent label.\\n\\n---\\n\\nFollow the following format.\\n\\nCustomer Message: Customer message during customer service interaction\\nIntent Labels: Labels that represent customer intent\\nIntent Class: a label best matching customer's intent\\n\\n---\\n\\nCustomer Message: I want to cancel my subscription\\nIntent Labels: Cancel subscription Refund request\\nIntent Class: Refund request\\n\\n---\\n\\nCustomer Message: I want to cancel my subscription\\nIntent Labels: Cancel subscription Refund request\\nIntent Class: Refund request\\n\\n---\\n\\nCustomer Message: I want to cancel my subscription\\nIntent Labels: Cancel subscription Refund request\\nIntent Class: Refund request\\n\\n---\\n\\nCustomer Message: I want to cancel my subscription\\nIntent Labels: Cancel subscription Refund request\\nIntent Class: Refund request\\n\\n---\\n\\nCustomer Message: I want to cancel my subscription\\nIntent Labels: Cancel subscription Refund request\\nIntent Class: Refund request\\n\\n---\\n\\nCustomer Message: I want to\"\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict = dspy.Predict(Classification)\n",
    "labels = ([\"Cancel subscription\", \"Refund request\"])\n",
    "labels = \" \".join([\"Cancel subscription\", \"Refund request\"])\n",
    "text = \"I want to cancel my subscription\"\n",
    "pred = predict(customer_message=text, intent_labels=labels)\n",
    "pred\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
